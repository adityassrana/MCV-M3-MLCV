{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-be2da956-e838-4ff2-930e-cfa88055e0fe","deepnote_to_be_reexecuted":false,"source_hash":"69f1c9a8","execution_start":1609527452658,"execution_millis":1430,"deepnote_cell_type":"code"},"source":"!pip install opencv-contrib-python==3.4.2.17","execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nRequirement already satisfied: opencv-contrib-python==3.4.2.17 in /opt/venv/lib/python3.7/site-packages (3.4.2.17)\nRequirement already satisfied: numpy>=1.14.5 in /opt/venv/lib/python3.7/site-packages (from opencv-contrib-python==3.4.2.17) (1.19.4)\n\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00000-005a6e5b-a0ca-4c6f-adf1-ac0d797617b8","deepnote_to_be_reexecuted":false,"source_hash":"31206826","execution_millis":614,"execution_start":1609527456458,"deepnote_cell_type":"code"},"source":"import cv2\nimport numpy as np\nimport pickle as cPickle\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Let us first read the train and test files","metadata":{"cell_id":"00001-ea41c09f-2341-4fa0-a281-879f6aa560d1","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00002-0c6f555e-30b9-4556-ba37-7d4a419949ae","deepnote_to_be_reexecuted":false,"source_hash":"fd3703c8","execution_millis":3,"execution_start":1609527459277,"deepnote_cell_type":"code"},"source":"train_images_filenames = cPickle.load(open('train_images_filenames.dat','rb'))\ntest_images_filenames = cPickle.load(open('test_images_filenames.dat','rb'))\ntrain_labels = cPickle.load(open('train_labels.dat','rb'))\ntest_labels = cPickle.load(open('test_labels.dat','rb'))","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00003-3a2c4782-cf5b-4d85-8fad-f6a36e1ac30e","deepnote_to_be_reexecuted":false,"source_hash":"e75bfe3","execution_millis":2,"execution_start":1609527461596,"deepnote_cell_type":"code"},"source":"train_images_filenames[12]","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"'../../Databases/MIT_split/train/Opencountry/fie26.jpg'"},"metadata":{}}]},{"cell_type":"markdown","source":"We create a SIFT object detector and descriptor","metadata":{"cell_id":"00004-f443db3d-7ab9-4f81-accc-3e966de59c72","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00005-6fc72511-03d9-447a-a817-e9aefa57cd04","deepnote_to_be_reexecuted":false,"source_hash":"30710540","execution_millis":1,"execution_start":1609527463557,"deepnote_cell_type":"code"},"source":"SIFTdetector = cv2.xfeatures2d.SIFT_create(nfeatures=300)","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We compute the SIFT descriptors for all the train images and subsequently build a numpy array with all the descriptors stacked together","metadata":{"cell_id":"00006-f4188841-4182-4518-933c-4885397466f3","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00007-1d4be4e3-cd5c-4e9f-98d9-959e7d7965f7","deepnote_to_be_reexecuted":false,"source_hash":"8a5098dd","execution_start":1609528672338,"execution_millis":71300,"deepnote_cell_type":"code"},"source":"Train_descriptors = []\nTrain_label_per_descriptor = []\n\nfor filename,labels in zip(train_images_filenames,train_labels):\n    ima=cv2.imread(filename)\n    gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n    kpt,des=SIFTdetector.detectAndCompute(gray,None)\n    Train_descriptors.append(des)\n    Train_label_per_descriptor.append(labels)\n\nD=np.vstack(Train_descriptors)","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We now compute a k-means clustering on the descriptor space","metadata":{"cell_id":"00008-2ab2d595-8bc1-4d73-890e-4d0c500ae0b9","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00009-4a3f7f1e-e45a-4fe4-b3b2-a6e8c131fd1e","deepnote_to_be_reexecuted":false,"source_hash":"cb136a19","execution_start":1609529101130,"execution_millis":28564,"deepnote_cell_type":"code"},"source":"k = 128\ncodebook = MiniBatchKMeans(n_clusters=k, verbose=False, batch_size=k * 20,compute_labels=False,reassignment_ratio=10**-4,random_state=42)\ncodebook.fit(D)","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"MiniBatchKMeans(batch_size=2560, compute_labels=False, n_clusters=128,\n                random_state=42, reassignment_ratio=0.0001, verbose=False)"},"metadata":{}}]},{"cell_type":"markdown","source":"And, for each train image, we project each keypoint descriptor to its closest visual word. We represent each of the images with the frequency of each visual word.","metadata":{"cell_id":"00010-cf7fcd24-b3b5-437b-bd0f-11f3612c9ed3","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00011-197d6e80-0dca-45b3-b394-d26c37d2bcf8","deepnote_to_be_reexecuted":false,"source_hash":"9b1db1f8","execution_millis":17383,"execution_start":1609529876931,"deepnote_cell_type":"code"},"source":"visual_words=np.zeros((len(Train_descriptors),k),dtype=np.float32)\nfor i in range(len(Train_descriptors)):\n    words=codebook.predict(Train_descriptors[i])\n    visual_words[i,:]=np.bincount(words,minlength=k)","execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"We build a k-nn classifier and train it with the train descriptors","metadata":{"cell_id":"00012-c254a6dc-35d7-4e8a-9e42-12afbfbeedcb","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00013-c7f6c336-0244-4b0b-aaa0-7759b1747c14","deepnote_to_be_reexecuted":false,"source_hash":"af24dcbf","execution_millis":17,"execution_start":1609529899412,"deepnote_cell_type":"code"},"source":"knn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1,metric='euclidean')\nknn.fit(visual_words, train_labels) ","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"KNeighborsClassifier(metric='euclidean', n_jobs=-1)"},"metadata":{}}]},{"cell_type":"markdown","source":"We end up computing the test descriptors and compute the accuracy of the model","metadata":{"cell_id":"00014-551eae35-7f48-4d70-af92-3217c052afa1","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00015-11f8ab73-adc0-4aca-81b5-cde59eb22005","deepnote_to_be_reexecuted":false,"source_hash":"ad0f5106","execution_start":1609529923284,"execution_millis":102826,"deepnote_cell_type":"code"},"source":"visual_words_test=np.zeros((len(test_images_filenames),k),dtype=np.float32)\nfor i in range(len(test_images_filenames)):\n    filename=test_images_filenames[i]\n    ima=cv2.imread(filename)\n    gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n    kpt,des=SIFTdetector.detectAndCompute(gray,None)\n    words=codebook.predict(des)\n    visual_words_test[i,:]=np.bincount(words,minlength=k)","execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00016-9a9742b7-158b-4cfa-9afb-3930eeb61968","deepnote_to_be_reexecuted":false,"source_hash":"8c41860f","execution_start":1609530032842,"execution_millis":278,"deepnote_cell_type":"code"},"source":"accuracy = 100*knn.score(visual_words_test, test_labels)\nprint(accuracy)","execution_count":27,"outputs":[{"name":"stdout","text":"51.92069392812887\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Dimensionality reduction, with PCA and LDA","metadata":{"cell_id":"00017-7a7d457a-25ef-482f-8426-1e1aeb3ecae7","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00018-956572ad-93da-4059-a90f-f5b73ae2407a","deepnote_to_be_reexecuted":false,"source_hash":"c9ed29f7","execution_start":1609530049660,"execution_millis":635,"deepnote_cell_type":"code"},"source":"pca = PCA(n_components=64)\nVWpca = pca.fit_transform(visual_words)\nknnpca = KNeighborsClassifier(n_neighbors=5,n_jobs=-1,metric='euclidean')\nknnpca.fit(VWpca, train_labels) \nvwtestpca = pca.transform(visual_words_test)\naccuracy = 100*knnpca.score(vwtestpca, test_labels)\nprint(accuracy)","execution_count":28,"outputs":[{"name":"stdout","text":"53.53159851301115\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00019-d135e675-030e-45c1-b32a-c0cbd05b6009","deepnote_to_be_reexecuted":false,"source_hash":"7b4d47be","execution_start":1609530055272,"execution_millis":11,"deepnote_cell_type":"code"},"source":"lda = LinearDiscriminantAnalysis(n_components=64)\nVWlda = lda.fit_transform(visual_words,train_labels)\nknnlda = KNeighborsClassifier(n_neighbors=5,n_jobs=-1,metric='euclidean')\nknnlda.fit(VWlda, train_labels) \nvwtestlda = lda.transform(visual_words_test)\naccuracy = 100*knnlda.score(vwtestlda, test_labels)\nprint(accuracy)","execution_count":29,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"n_components cannot be larger than min(n_features, n_classes - 1).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-c0d6d4899dae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mVWlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mknnlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mknnlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVWlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvwtestlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_words_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/sklearn/discriminant_analysis.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;34m\"n_components cannot be larger than min(n_features, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                     \u001b[0;34m\"n_classes - 1).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 )\n","\u001b[0;31mValueError\u001b[0m: n_components cannot be larger than min(n_features, n_classes - 1)."]}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.15rc1"},"deepnote_notebook_id":"4e1efd95-dca6-479f-8cc7-63e70b1151f2","deepnote_execution_queue":[]}}